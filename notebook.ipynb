{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "tar_file = r'D:\\PyCharmMiscProject\\slo.zip'\n",
    "\n",
    "extract_dir = 'working'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(tar_file, 'r') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:01.360490Z",
     "start_time": "2025-07-25T21:00:48.961464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install hf_xet"
   ],
   "id": "df1c6ebc17789a05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in d:\\pythonproject\\.venv\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from transformers[torch]) (1.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\pythonproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
      "Requirement already satisfied: psutil in d:\\pythonproject\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\pythonproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\pythonproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\pythonproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\pythonproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2025.7.14)\n",
      "Requirement already satisfied: datasets in d:\\pythonproject\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\pythonproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in d:\\pythonproject\\.venv\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pythonproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in d:\\pythonproject\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\pythonproject\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\pythonproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\pythonproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pythonproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: soundfile in d:\\pythonproject\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\pythonproject\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in d:\\pythonproject\\.venv\\lib\\site-packages (from soundfile) (2.3.1)\n",
      "Requirement already satisfied: pycparser in d:\\pythonproject\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: hf_xet in d:\\pythonproject\\.venv\\lib\\site-packages (1.1.5)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:02.684149Z",
     "start_time": "2025-07-25T21:01:01.384376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\"working\")\n",
    "metadata_files = list(base_path.rglob(\"*.tsv\"))\n",
    "\n",
    "train_path = next(p for p in metadata_files if \"train\" in p.name)\n",
    "validated_path = next(p for p in metadata_files if \"validated\" in p.name)\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "validated_df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "main_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "def ensure_mp3_extension(p):\n",
    "    return p if p.endswith(\".mp3\") else p + \".mp3\"\n",
    "\n",
    "main_df[\"path\"] = main_df[\"path\"].apply(ensure_mp3_extension)\n",
    "\n",
    "clips_dir = base_path / \"clips\"\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: (clips_dir / p).as_posix())\n",
    "\n",
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "valid_df = main_df[main_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid audio samples: {len(valid_df)}\")"
   ],
   "id": "1fa438bed83b9215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid audio samples: 1307\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:14.251502Z",
     "start_time": "2025-07-25T21:01:02.710274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    " Wav2Vec2ForCTC,TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\n",
    "def prepare_dataset(df, sampling_rate=16000):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df = df.rename(columns={'audio_path': 'audio', 'sentence': 'text'})\n",
    "    ds = Dataset.from_pandas(df[[\"audio\", \"text\"]])\n",
    "    return ds"
   ],
   "id": "5152095a11e16095",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:14.319354Z",
     "start_time": "2025-07-25T21:01:14.285109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def load_audio_pydub(path, target_sampling_rate=16000):\n",
    "    audio = AudioSegment.from_file(path)\n",
    "    if audio.frame_rate != target_sampling_rate:\n",
    "        audio = audio.set_frame_rate(target_sampling_rate)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples, target_sampling_rate"
   ],
   "id": "3dd2dc8069360463",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:16.919171Z",
     "start_time": "2025-07-25T21:01:14.363957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n",
    "\n",
    "def preprocess_single_example(example, processor, sampling_rate=16000):\n",
    "        from pydub import AudioSegment\n",
    "        import numpy as np\n",
    "\n",
    "        path = example[\"audio\"]\n",
    "        text = example[\"text\"]\n",
    "\n",
    "        audio = AudioSegment.from_file(path)\n",
    "        if audio.frame_rate != sampling_rate:\n",
    "            audio = audio.set_frame_rate(sampling_rate)\n",
    "        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "        if audio.channels > 1:\n",
    "            samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "\n",
    "        inputs = processor(samples, sampling_rate=sampling_rate, return_attention_mask=True, padding=True)\n",
    "\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(text).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values[0],\n",
    "            \"attention_mask\": inputs.attention_mask[0],\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "id": "d569a00d1a7acf94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:16.952286Z",
     "start_time": "2025-07-25T21:01:16.943973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "        processor: Wav2Vec2Processor\n",
    "        padding: Union[bool, str] = True\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_features = [{\"input_values\":f[\"input_values\"]} for f in features]\n",
    "            label_features = [{\"input_ids\":f[\"labels\"]} for f in features]\n",
    "\n",
    "            batch = self.processor.pad(\n",
    "                input_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with self.processor.as_target_processor():\n",
    "                labels_batch = self.processor.pad(\n",
    "                    label_features,\n",
    "                    padding=self.padding,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "            labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = labels\n",
    "\n",
    "            return batch"
   ],
   "id": "ec834c775e79f2df",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "    class Config:\n",
    "        MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "        SAMPLING_RATE = 16000\n",
    "        NUM_EPOCHS = 2\n",
    "        BATCH_SIZE = 4\n",
    "        OUTPUT_DIR = \"./wav2vec2-test\"\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_NAME)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    print(\"Processing dataset...\")\n",
    "    processed_dataset = small_dataset.map(\n",
    "        lambda x: preprocess_single_example(x, processor),\n",
    "        remove_columns=small_dataset.column_names,\n",
    "        desc=\"Processing audio files\"\n",
    "    ).filter(lambda x: x is not None)\n",
    "\n",
    "    print(f\"Processed {len(processed_dataset)} examples\")\n",
    "\n",
    "    split = processed_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_dir=f\"{Config.OUTPUT_DIR}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=False,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    return trainer, processor"
   ],
   "id": "22ab80cf6717f2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:01:21.955202Z",
     "start_time": "2025-07-25T21:01:16.981055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, vocab_size=len(processor.tokenizer), ignore_mismatched_sizes=True)\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "26d80235de8b4928",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.195067Z",
     "start_time": "2025-07-25T21:01:21.978037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = prepare_dataset(valid_df)\n",
    "ds = dataset.map(lambda x: preprocess_single_example(x, processor), remove_columns=dataset.column_names)"
   ],
   "id": "de558bfc2d5d119a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1307/1307 [03:05<00:00,  7.06 examples/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.277974Z",
     "start_time": "2025-07-25T21:04:27.263506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_ds = ds.train_test_split(test_size=0.1)\n",
    "train_ds = split_ds[\"train\"]\n",
    "eval_ds = split_ds[\"test\"]"
   ],
   "id": "7bbe0fb54aa7b9b9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.311056Z",
     "start_time": "2025-07-25T21:04:27.292016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-slovene\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    learning_rate=3e-4,\n",
    "    save_total_limit=2,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ],
   "id": "7c8c6e0bd7791e3c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.366773Z",
     "start_time": "2025-07-25T21:04:27.361938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer(label_str, pred_str),\n",
    "        \"cer\": cer(label_str, pred_str)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n"
   ],
   "id": "28b9591834b86437",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.433693Z",
     "start_time": "2025-07-25T21:04:27.400399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "396f25c197721735",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\AppData\\Local\\Temp\\ipykernel_13256\\448576995.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./wav2vec2-slovene\")\n",
    "processor.save_pretrained(\"./wav2vec2-slovene\")"
   ],
   "id": "d7ed141b1ff39c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ],
   "id": "8a0cccd3ba67c5fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "86b1f9d6ad147c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:27.473543Z",
     "start_time": "2025-07-25T21:04:27.467723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "from pydub import AudioSegment\n",
    "from jiwer import wer\n"
   ],
   "id": "9fc1a50b5ef3f9db",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:30.997986Z",
     "start_time": "2025-07-25T21:04:27.495553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "991caae48f1e6dd9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T21:04:31.041800Z",
     "start_time": "2025-07-25T21:04:31.034780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_audio(path):\n",
    "    audio = AudioSegment.from_file(path).set_frame_rate(16000)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples"
   ],
   "id": "54efaca3f0e9933c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:26:26.787587Z",
     "start_time": "2025-07-25T22:26:26.782332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(batch):\n",
    "    audio_inputs = [load_audio(p) for p in batch[\"audio_path\"]]\n",
    "\n",
    "    inputs = processor.feature_extractor(\n",
    "        audio_inputs,\n",
    "        sampling_rate=16000,\n",
    "        return_attention_mask=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label_batch = processor.tokenizer(\n",
    "            batch[\"sentence\"],\n",
    "            padding=True,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "\n",
    "    labels = label_batch[\"input_ids\"]\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n"
   ],
   "id": "10d8ed7cb56b0ec2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:29:15.772923Z",
     "start_time": "2025-07-25T22:26:27.327251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = Dataset.from_pandas(valid_df[[\"audio_path\", \"sentence\"]])\n",
    "ds = ds.map(preprocess, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "split = ds.train_test_split(test_size=0.1)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]"
   ],
   "id": "cad3b6173227198f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1307/1307 [02:48<00:00,  7.76 examples/s]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:29:24.134172Z",
     "start_time": "2025-07-25T22:29:24.104417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: Wav2Vec2Processor\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input = self.processor.pad([{\"input_values\": f[\"input_values\"]} for f in features], return_tensors=\"pt\")\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor.pad([{\"input_ids\": f[\"labels\"]} for f in features], return_tensors=\"pt\")\n",
    "        input[\"labels\"] = labels[\"input_ids\"].masked_fill(labels[\"attention_mask\"].ne(1), -100)\n",
    "        return input"
   ],
   "id": "11c4364423c3a229",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:29:24.824101Z",
     "start_time": "2025-07-25T22:29:24.815068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    for ref, hyp in zip(label_str[:5], pred_str[:5]):\n",
    "        print(f\"REF : {ref}\")\n",
    "        print(f\"PRED: {hyp}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    error = wer(label_str, pred_str)\n",
    "    return {\"wer\": error}"
   ],
   "id": "149dec59af78e459",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T22:46:13.876318Z",
     "start_time": "2025-07-25T22:29:26.536504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./slovene-model\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        eval_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=1,\n",
    "        report_to=[]\n",
    "    ),\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=DataCollator(processor),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ],
   "id": "e2a2d18cb9f1fec3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\AppData\\Local\\Temp\\ipykernel_13256\\704549770.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21/882 11:26 < 8:38:45, 0.03 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.444800</td>\n",
       "      <td>3.359293</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF : edno zaprite vrata hlevov\n",
      "PRED: \n",
      "-----\n",
      "REF : jubezen je kot ogenj kolikor je večja toliko bolj se kadi\n",
      "PRED: \n",
      "-----\n",
      "REF : sak novorojenček je lep vsak ženin je dober vsak pokojni je svetnik\n",
      "PRED: \n",
      "-----\n",
      "REF : naki pari najbolje plešejo\n",
      "PRED: \n",
      "-----\n",
      "REF : dor se je naredil za osla naj se ne čudi če ga drugi jahajo\n",
      "PRED: \n",
      "-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m      1\u001B[39m trainer = Trainer(\n\u001B[32m      2\u001B[39m     model=model,\n\u001B[32m      3\u001B[39m     args=TrainingArguments(\n\u001B[32m   (...)\u001B[39m\u001B[32m     18\u001B[39m     compute_metrics=compute_metrics\n\u001B[32m     19\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(trainer.evaluate())\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2206\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2204\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2205\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2206\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2207\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2209\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2210\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2211\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2541\u001B[39m context = (\n\u001B[32m   2542\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2543\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2544\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2545\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2546\u001B[39m )\n\u001B[32m   2547\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2548\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2551\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2552\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2553\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2554\u001B[39m ):\n\u001B[32m   2555\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2556\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3797\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   3794\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   3795\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3797\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3799\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2578\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2576\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2577\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2578\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:307\u001B[39m, in \u001B[36mBackwardCFunction.apply\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    301\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    302\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mImplementing both \u001B[39m\u001B[33m'\u001B[39m\u001B[33mbackward\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33mvjp\u001B[39m\u001B[33m'\u001B[39m\u001B[33m for a custom \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    303\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFunction is not allowed. You should only implement one \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    304\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mof them.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    305\u001B[39m     )\n\u001B[32m    306\u001B[39m user_fn = vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function.vjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43muser_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:320\u001B[39m, in \u001B[36mCheckpointFunction.backward\u001B[39m\u001B[34m(ctx, *args)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs_with_grad) == \u001B[32m0\u001B[39m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    317\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mnone of output has requires_grad=True,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    318\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m this checkpoint() is not necessary\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    319\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m320\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs_with_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs_with_grad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    321\u001B[39m grads = \u001B[38;5;28mtuple\u001B[39m(\n\u001B[32m    322\u001B[39m     inp.grad \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inp, torch.Tensor) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m inp \u001B[38;5;129;01min\u001B[39;00m detached_inputs\n\u001B[32m    324\u001B[39m )\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m) + grads\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(processor.tokenizer.get_vocab().keys())",
   "id": "2f22d6d213bdbc3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(dataset.column_names)",
   "id": "a8a26d13cbfafbd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b7ce786eac4379d1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
