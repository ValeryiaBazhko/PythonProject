{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "tar_file = r'D:\\PyCharmMiscProject\\slo.zip'\n",
    "\n",
    "extract_dir = 'working'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(tar_file, 'r') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install hf_xet"
   ],
   "id": "df1c6ebc17789a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:32.248979Z",
     "start_time": "2025-07-24T20:42:29.141375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\"working\")\n",
    "metadata_files = list(base_path.rglob(\"*.tsv\"))\n",
    "\n",
    "train_path = next(p for p in metadata_files if \"train\" in p.name)\n",
    "validated_path = next(p for p in metadata_files if \"validated\" in p.name)\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "validated_df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "main_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "def ensure_mp3_extension(p):\n",
    "    return p if p.endswith(\".mp3\") else p + \".mp3\"\n",
    "\n",
    "main_df[\"path\"] = main_df[\"path\"].apply(ensure_mp3_extension)\n",
    "\n",
    "clips_dir = base_path / \"clips\"\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: (clips_dir / p).as_posix())\n",
    "\n",
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "valid_df = main_df[main_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid audio samples: {len(valid_df)}\")"
   ],
   "id": "1fa438bed83b9215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid audio samples: 1307\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:58.797104Z",
     "start_time": "2025-07-24T20:42:34.323617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    " Wav2Vec2ForCTC,TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\n",
    "def prepare_dataset(df, sampling_rate=16000):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df = df.rename(columns={'audio_path': 'audio', 'sentence': 'text'})\n",
    "    ds = Dataset.from_pandas(df[[\"audio\", \"text\"]])\n",
    "    return ds"
   ],
   "id": "5152095a11e16095",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:58.841220Z",
     "start_time": "2025-07-24T20:42:58.815633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def load_audio_pydub(path, target_sampling_rate=16000):\n",
    "    audio = AudioSegment.from_file(path)\n",
    "    if audio.frame_rate != target_sampling_rate:\n",
    "        audio = audio.set_frame_rate(target_sampling_rate)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples, target_sampling_rate"
   ],
   "id": "3dd2dc8069360463",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:00.938230Z",
     "start_time": "2025-07-24T20:42:58.860221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n",
    "\n",
    "def preprocess_single_example(example, processor, sampling_rate=16000):\n",
    "        from pydub import AudioSegment\n",
    "        import numpy as np\n",
    "\n",
    "        path = example[\"audio\"]\n",
    "        text = example[\"text\"]\n",
    "\n",
    "        audio = AudioSegment.from_file(path)\n",
    "        if audio.frame_rate != sampling_rate:\n",
    "            audio = audio.set_frame_rate(sampling_rate)\n",
    "        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "        if audio.channels > 1:\n",
    "            samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "\n",
    "        inputs = processor(samples, sampling_rate=sampling_rate, return_attention_mask=True, padding=True)\n",
    "\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(text).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values[0],\n",
    "            \"attention_mask\": inputs.attention_mask[0],\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "id": "d569a00d1a7acf94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:00.968062Z",
     "start_time": "2025-07-24T20:43:00.961771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "        processor: Wav2Vec2Processor\n",
    "        padding: Union[bool, str] = True\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_features = [{\"input_values\":f[\"input_values\"]} for f in features]\n",
    "            label_features = [{\"input_ids\":f[\"labels\"]} for f in features]\n",
    "\n",
    "            batch = self.processor.pad(\n",
    "                input_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with self.processor.as_target_processor():\n",
    "                labels_batch = self.processor.pad(\n",
    "                    label_features,\n",
    "                    padding=self.padding,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "            labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = labels\n",
    "\n",
    "            return batch"
   ],
   "id": "ec834c775e79f2df",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "    class Config:\n",
    "        MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "        SAMPLING_RATE = 16000\n",
    "        NUM_EPOCHS = 2\n",
    "        BATCH_SIZE = 4\n",
    "        OUTPUT_DIR = \"./wav2vec2-test\"\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_NAME)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    print(\"Processing dataset...\")\n",
    "    processed_dataset = small_dataset.map(\n",
    "        lambda x: preprocess_single_example(x, processor),\n",
    "        remove_columns=small_dataset.column_names,\n",
    "        desc=\"Processing audio files\"\n",
    "    ).filter(lambda x: x is not None)\n",
    "\n",
    "    print(f\"Processed {len(processed_dataset)} examples\")\n",
    "\n",
    "    split = processed_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_dir=f\"{Config.OUTPUT_DIR}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=False,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    return trainer, processor"
   ],
   "id": "22ab80cf6717f2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:05.632630Z",
     "start_time": "2025-07-24T20:43:00.997679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, vocab_size=len(processor.tokenizer), ignore_mismatched_sizes=True)\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "26d80235de8b4928",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-24T20:43:05.649636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = prepare_dataset(valid_df)\n",
    "ds = dataset.map(lambda x: preprocess_single_example(x, processor), remove_columns=dataset.column_names)"
   ],
   "id": "de558bfc2d5d119a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map:   6%|â–Œ         | 74/1307 [00:11<03:03,  6.71 examples/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "split_ds = ds.train_test_split(test_size=0.1)\n",
    "train_ds = split_ds[\"train\"]\n",
    "eval_ds = split_ds[\"test\"]"
   ],
   "id": "7bbe0fb54aa7b9b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-slovene\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    learning_rate=3e-4,\n",
    "    save_total_limit=2,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ],
   "id": "7c8c6e0bd7791e3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer(label_str, pred_str),\n",
    "        \"cer\": cer(label_str, pred_str)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n"
   ],
   "id": "28b9591834b86437",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "396f25c197721735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./wav2vec2-slovene\")\n",
    "processor.save_pretrained(\"./wav2vec2-slovene\")"
   ],
   "id": "d7ed141b1ff39c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ],
   "id": "8a0cccd3ba67c5fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "86b1f9d6ad147c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "from pydub import AudioSegment\n",
    "from jiwer import wer\n"
   ],
   "id": "9fc1a50b5ef3f9db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "991caae48f1e6dd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_audio(path):\n",
    "    audio = AudioSegment.from_file(path).set_frame_rate(16000)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples"
   ],
   "id": "54efaca3f0e9933c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess(batch):\n",
    "    audio = load_audio(batch[\"audio_path\"])\n",
    "    inputs = processor(audio, sampling_rate=16000, return_attention_mask=True)\n",
    "    with processor.as_target_processor():\n",
    "        labels = processor(batch[\"sentence\"]).input_ids\n",
    "    return {\n",
    "        \"input_values\": inputs[\"input_values\"][0],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "id": "10d8ed7cb56b0ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds = Dataset.from_pandas(valid_df[[\"audio_path\", \"sentence\"]])\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "split = ds.train_test_split(test_size=0.1)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]"
   ],
   "id": "cad3b6173227198f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: Wav2Vec2Processor\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input = self.processor.pad([{\"input_values\": f[\"input_values\"]} for f in features], return_tensors=\"pt\")\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor.pad([{\"input_ids\": f[\"labels\"]} for f in features], return_tensors=\"pt\")\n",
    "        input[\"labels\"] = labels[\"input_ids\"].masked_fill(labels[\"attention_mask\"].ne(1), -100)\n",
    "        return input"
   ],
   "id": "11c4364423c3a229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": wer(label_str, pred_str)}"
   ],
   "id": "149dec59af78e459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./slovene-model\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        eval_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=1,\n",
    "        report_to=[]\n",
    "    ),\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=DataCollator(processor),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ],
   "id": "e2a2d18cb9f1fec3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f22d6d213bdbc3c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
