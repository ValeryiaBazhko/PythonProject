{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T18:36:49.624293Z",
     "start_time": "2025-07-21T18:36:44.610666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "tar_file = 'sl.tar'\n",
    "\n",
    "\n",
    "extract_dir = 'working'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with tarfile.open(tar_file, 'r') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T18:36:56.100569Z",
     "start_time": "2025-07-21T18:36:49.890946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install hf_xet\n"
   ],
   "id": "3a43386501076567",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[torch]) (1.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2025.7.14)\n",
      "Requirement already satisfied: datasets in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from soundfile) (2.2.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: hf_xet in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (1.1.5)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:34:35.634858Z",
     "start_time": "2025-07-21T19:34:34.989914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\"working\")\n",
    "metadata_files = list(base_path.rglob(\"*.tsv\"))\n",
    "\n",
    "train_path = next(p for p in metadata_files if \"train\" in p.name)\n",
    "validated_path = next(p for p in metadata_files if \"validated\" in p.name)\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "validated_df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "main_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "def ensure_mp3_extension(p):\n",
    "    return p if p.endswith(\".mp3\") else p + \".mp3\"\n",
    "\n",
    "main_df[\"path\"] = main_df[\"path\"].apply(ensure_mp3_extension)\n",
    "\n",
    "clips_dir = base_path / \"clips\"\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: (clips_dir / p).as_posix())\n",
    "\n",
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "valid_df = main_df[main_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid audio samples: {len(valid_df)}\")\n"
   ],
   "id": "af50c9a68afc198d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid audio samples: 1307\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:34:37.822085Z",
     "start_time": "2025-07-21T19:34:36.657144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(df, sampling_rate=16000):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df = df.rename(columns={'audio_path': 'audio', 'sentence': 'text'})\n",
    "    ds = Dataset.from_pandas(df[[\"audio\", \"text\"]])\n",
    "    return ds\n"
   ],
   "id": "6f5001c5914469c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:34:38.497201Z",
     "start_time": "2025-07-21T19:34:38.479675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def load_audio_pydub(path, target_sampling_rate=16000):\n",
    "    audio = AudioSegment.from_file(path)\n",
    "    if audio.frame_rate != target_sampling_rate:\n",
    "        audio = audio.set_frame_rate(target_sampling_rate)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples, target_sampling_rate"
   ],
   "id": "33417efe7f9c10cf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:37:26.536350Z",
     "start_time": "2025-07-21T19:34:40.039359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n",
    "\n",
    "def preprocess_dataset(batch, processor):\n",
    "    input_values = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    for audio_path, text in zip(batch[\"audio\"], batch[\"text\"]):\n",
    "        speech_array, sampling_rate = load_audio_pydub(audio_path)\n",
    "\n",
    "        inputs = processor(speech_array, sampling_rate=sampling_rate, return_attention_mask=True, padding=True)\n",
    "        input_values.append(inputs[\"input_values\"][0])\n",
    "        attention_mask.append(inputs[\"attention_mask\"][0])\n",
    "\n",
    "        with processor.as_target_processor():\n",
    "            label_ids = processor(text).input_ids\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "dataset = prepare_dataset(valid_df, sampling_rate=16000)\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda batch: preprocess_dataset(batch, processor),\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    batch_size=8,\n",
    ")"
   ],
   "id": "b9e30d02a0cda967",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1307/1307 [02:35<00:00,  8.42 examples/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:37:37.741115Z",
     "start_time": "2025-07-21T19:37:32.654800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install protobuf\n",
    "!pip install soundfile datasets transformers[sentencepiece]\n",
    "!pip install --upgrade transformers\n",
    "def quick_test_training(dataset, max_samples=100):\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "    class Config:\n",
    "        MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "        SAMPLING_RATE = 16000\n",
    "        NUM_EPOCHS = 2\n",
    "        BATCH_SIZE = 8\n",
    "        OUTPUT_DIR = \"./wav2vec2-test\"\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_NAME)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(Config.MODEL_NAME,ignore_mismatched_sizes=True, vocab_size=len(processor.tokenizer))\n",
    "\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    processed_dataset = dataset.map(\n",
    "    lambda x: preprocess_dataset(x, processor),\n",
    "    remove_columns=dataset.column_names\n",
    ").filter(lambda x: x is not None)\n",
    "\n",
    "    split = processed_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        save_steps=50,\n",
    "        eval_steps=50,\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor\n",
    "    )\n",
    "\n",
    "    print(\"Columns after preprocessing:\", processed_dataset.column_names)\n",
    "    print(\"First item:\", processed_dataset[0])\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    return trainer, processor\n"
   ],
   "id": "c634cc9654023c78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (6.31.1)\n",
      "Requirement already satisfied: soundfile in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from soundfile) (2.2.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (6.31.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lerab\\pycharmmiscproject\\.venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:37:46.925757Z",
     "start_time": "2025-07-21T19:37:42.542729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"TrainingArguments location:\", transformers.TrainingArguments.__module__)\n",
    "dataset = prepare_dataset(valid_df, sampling_rate=16000)\n",
    "trainer, processor = quick_test_training(dataset)"
   ],
   "id": "5681c5826ed14052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.53.2\n",
      "TrainingArguments location: transformers.training_args\n",
      "Running quick test with 100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'w'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTrainingArguments location:\u001B[39m\u001B[33m\"\u001B[39m, transformers.TrainingArguments.\u001B[34m__module__\u001B[39m)\n\u001B[32m      4\u001B[39m dataset = prepare_dataset(valid_df, sampling_rate=\u001B[32m16000\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m trainer, processor = \u001B[43mquick_test_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mquick_test_training\u001B[39m\u001B[34m(dataset, max_samples)\u001B[39m\n\u001B[32m     19\u001B[39m     model = Wav2Vec2ForCTC.from_pretrained(Config.MODEL_NAME,ignore_mismatched_sizes=\u001B[38;5;28;01mTrue\u001B[39;00m, vocab_size=\u001B[38;5;28mlen\u001B[39m(processor.tokenizer))\n\u001B[32m     21\u001B[39m     model.freeze_feature_encoder()\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     processed_dataset = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumn_names\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m)\u001B[49m.filter(\u001B[38;5;28;01mlambda\u001B[39;00m x: x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     28\u001B[39m     split = processed_dataset.train_test_split(test_size=\u001B[32m0.1\u001B[39m)\n\u001B[32m     29\u001B[39m     train_dataset = split[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001B[39m, in \u001B[36mtransmit_format.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    553\u001B[39m self_format = {\n\u001B[32m    554\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_type,\n\u001B[32m    555\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mformat_kwargs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_kwargs,\n\u001B[32m    556\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_columns,\n\u001B[32m    557\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput_all_columns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._output_all_columns,\n\u001B[32m    558\u001B[39m }\n\u001B[32m    559\u001B[39m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m560\u001B[39m out: Union[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDatasetDict\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    561\u001B[39m datasets: \u001B[38;5;28mlist\u001B[39m[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mlist\u001B[39m(out.values()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[32m    562\u001B[39m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3318\u001B[39m, in \u001B[36mDataset.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001B[39m\n\u001B[32m   3316\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3317\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m unprocessed_kwargs \u001B[38;5;129;01min\u001B[39;00m unprocessed_kwargs_per_job:\n\u001B[32m-> \u001B[39m\u001B[32m3318\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43munprocessed_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3319\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcheck_if_shard_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3321\u001B[39m \u001B[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3650\u001B[39m, in \u001B[36mDataset._map_single\u001B[39m\u001B[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001B[39m\n\u001B[32m   3648\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batched:\n\u001B[32m   3649\u001B[39m     _time = time.time()\n\u001B[32m-> \u001B[39m\u001B[32m3650\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miter_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard_iterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3651\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mupdate_data\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3652\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3624\u001B[39m, in \u001B[36mDataset._map_single.<locals>.iter_outputs\u001B[39m\u001B[34m(shard_iterable)\u001B[39m\n\u001B[32m   3622\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3623\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[32m-> \u001B[39m\u001B[32m3624\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m i, \u001B[43mapply_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001B[39m, in \u001B[36mDataset._map_single.<locals>.apply_function\u001B[39m\u001B[34m(pa_inputs, indices, offset)\u001B[39m\n\u001B[32m   3545\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001B[39;00m\n\u001B[32m   3546\u001B[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001B[32m-> \u001B[39m\u001B[32m3547\u001B[39m processed_inputs = \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3548\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 24\u001B[39m, in \u001B[36mquick_test_training.<locals>.<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     19\u001B[39m     model = Wav2Vec2ForCTC.from_pretrained(Config.MODEL_NAME,ignore_mismatched_sizes=\u001B[38;5;28;01mTrue\u001B[39;00m, vocab_size=\u001B[38;5;28mlen\u001B[39m(processor.tokenizer))\n\u001B[32m     21\u001B[39m     model.freeze_feature_encoder()\n\u001B[32m     23\u001B[39m     processed_dataset = dataset.map(\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mpreprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m     25\u001B[39m     remove_columns=dataset.column_names\n\u001B[32m     26\u001B[39m ).filter(\u001B[38;5;28;01mlambda\u001B[39;00m x: x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     28\u001B[39m     split = processed_dataset.train_test_split(test_size=\u001B[32m0.1\u001B[39m)\n\u001B[32m     29\u001B[39m     train_dataset = split[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mpreprocess_dataset\u001B[39m\u001B[34m(batch, processor)\u001B[39m\n\u001B[32m      8\u001B[39m labels = []\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m audio_path, text \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(batch[\u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m], batch[\u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m]):\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     speech_array, sampling_rate = \u001B[43mload_audio_pydub\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m     inputs = processor(speech_array, sampling_rate=sampling_rate, return_attention_mask=\u001B[38;5;28;01mTrue\u001B[39;00m, padding=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     15\u001B[39m     input_values.append(inputs[\u001B[33m\"\u001B[39m\u001B[33minput_values\u001B[39m\u001B[33m\"\u001B[39m][\u001B[32m0\u001B[39m])\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mload_audio_pydub\u001B[39m\u001B[34m(path, target_sampling_rate)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_audio_pydub\u001B[39m(path, target_sampling_rate=\u001B[32m16000\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     audio = \u001B[43mAudioSegment\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m audio.frame_rate != target_sampling_rate:\n\u001B[32m      7\u001B[39m         audio = audio.set_frame_rate(target_sampling_rate)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pydub\\audio_segment.py:651\u001B[39m, in \u001B[36mAudioSegment.from_file\u001B[39m\u001B[34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001B[39m\n\u001B[32m    649\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m    650\u001B[39m     filename = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m651\u001B[39m file, close_file = \u001B[43m_fd_or_path_or_tempfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtempfile\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m:\n\u001B[32m    654\u001B[39m     \u001B[38;5;28mformat\u001B[39m = \u001B[38;5;28mformat\u001B[39m.lower()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\pydub\\utils.py:60\u001B[39m, in \u001B[36m_fd_or_path_or_tempfile\u001B[39m\u001B[34m(fd, mode, tempfile)\u001B[39m\n\u001B[32m     57\u001B[39m     close_fd = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fd, basestring):\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     fd = \u001B[38;5;28mopen\u001B[39m(fd, mode=mode)\n\u001B[32m     61\u001B[39m     close_fd = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'w'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n"
   ],
   "id": "56292e8000e26c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(output_dir=\"./test\")\n",
    "print(args)"
   ],
   "id": "eb86e8aa03434a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ],
   "id": "28809b43a6762129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Union[int, None] = None\n",
    "    max_length_labels: Union[int, None] = None\n",
    "    pad_to_multiple_of: Union[int, None] = None\n",
    "    pad_to_multiple_of_labels: Union[int, None] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ],
   "id": "1344adf1de2d2be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_noise(audio, noise_factor=0.005):\n",
    "    \"\"\"Add noise to audio for data augmentation\"\"\"\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + noise_factor * noise"
   ],
   "id": "91a3a259e77f5f1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def speed_change(audio, factor=None):\n",
    "    \"\"\"Change speed of audio\"\"\"\n",
    "    if factor is None:\n",
    "        factor = np.random.uniform(0.9, 1.1) # change the values to try\n",
    "    indices = np.round(np.arange(0, len(audio), factor)).astype(int)\n",
    "    indices = indices[indices < len(audio)]\n",
    "    return audio[indices]"
   ],
   "id": "d04da775821e657d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(examples, processor, augment=False):\n",
    "    \"\"\"Preprocess examples\"\"\"\n",
    "\n",
    "    input_values = []\n",
    "    labels = []\n",
    "\n",
    "    for audio_data, text in zip(examples[\"audio\"], examples[\"text\"]):\n",
    "        try:\n",
    "            audio_array = audio_data[\"array\"]\n",
    "\n",
    "            if augment and np.random.random() > 0.5:\n",
    "                aug_type = np.random.choice(['noise', 'speed'])\n",
    "                if aug_type == 'noise':\n",
    "                    audio_array = add_noise(audio_array, noise_factor=0.005)\n",
    "                elif aug_type == 'speed':\n",
    "                    audio_array = speed_change(audio_array)\n",
    "\n",
    "            if len(audio_array) > Config.MAX_INPUT_LENGTH:\n",
    "                audio_array = audio_array[:Config.MAX_INPUT_LENGTH]\n",
    "\n",
    "            input_values.append(audio_array)\n",
    "\n",
    "            normalized_text = normalize_slovenian_text(text)\n",
    "            labels.append(normalized_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(input_values) == 0:\n",
    "        return {\"input_values\": [], \"labels\": []}\n",
    "\n",
    "    inputs = processor(\n",
    "        input_values,\n",
    "        sampling_rate=Config.SAMPLING_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=Config.MAX_INPUT_LENGTH\n",
    "    )\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label_features = processor(labels).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_values\": inputs.input_values,\n",
    "        \"labels\": label_features\n",
    "    }\n"
   ],
   "id": "684eb105883ae4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_results_to_notebook():\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if os.path.exists(\"/working/wav2vec2-results/training_history.csv\"):\n",
    "        df = pd.read_csv(\"/working/wav2vec2-results/training_history.csv\")\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(df['train_loss'], label='Train Loss')\n",
    "        plt.plot(df['eval_loss'], label='Eval Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(df['learning_rate'])\n",
    "        plt.title('Learning Rate')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Final Train Loss: {df['train_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Final Eval Loss: {df['eval_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Total Steps: {len(df)}\")"
   ],
   "id": "d57cd62a75c9e9d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_the_dataset(dataset):\n",
    "\n",
    "    print(f\"Starting training with {len(dataset)} samples\")\n",
    "\n",
    "    train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test[\"train\"]\n",
    "    eval_dataset = train_test[\"test\"]\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "    vocab_dict = create_vocabulary_from_dataset(train_dataset)\n",
    "\n",
    "    import os\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    with open(f\"{Config.OUTPUT_DIR}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_file=f\"{Config.OUTPUT_DIR}/vocab.json\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        word_delimiter_token=\" \"\n",
    "    )\n",
    "\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=Config.SAMPLING_RATE,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    processor = Wav2Vec2Processor(\n",
    "        feature_extractor=feature_extractor,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Preprocessing training data...\")\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        return preprocess_dataset(examples, processor, augment=True)\n",
    "\n",
    "    def preprocess_eval(examples):\n",
    "        return preprocess_dataset(examples, processor, augment=False)\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_train,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_eval,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            Config.MODEL_NAME,\n",
    "            attention_dropout=0.1,\n",
    "            hidden_dropout=0.1,\n",
    "            feat_proj_dropout=0.0,\n",
    "            mask_time_prob=0.05,\n",
    "            layerdrop=0.1,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(processor.tokenizer),\n",
    "            ctc_zero_infinity=True\n",
    "        )\n",
    "\n",
    "    model.freeze_feature_extractor()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=Config.OUTPUT_DIR,\n",
    "            group_by_length=True,\n",
    "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            eval_strategy=\"steps\",\n",
    "            num_train_epochs=Config.NUM_EPOCHS,\n",
    "            fp16=True,\n",
    "            save_steps=Config.SAVE_STEPS,\n",
    "            eval_steps=Config.EVAL_STEPS,\n",
    "            logging_steps=100,\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            weight_decay=0.005,\n",
    "            warmup_steps=Config.WARMUP_STEPS,\n",
    "            save_total_limit=3,\n",
    "            push_to_hub=False,\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "        )\n",
    "\n",
    "    data_collator = DataCollator(processor=processor, padding=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(Config.OUTPUT_DIR)\n",
    "\n",
    "    save_results_to_notebook()\n",
    "\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"Model saved to: {Config.OUTPUT_DIR}\")\n",
    "\n",
    "    return trainer, processor"
   ],
   "id": "147e0df41855e092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(processor_path, model_path, test_audio_path):\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "\n",
    "    audio, sr = librosa.load(test_audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription"
   ],
   "id": "da05a6f0538f7386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_visual_results_dashboard(trainer, processor, vocab_dict, config):\n",
    "\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        logs = trainer.state.log_history\n",
    "\n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        learning_rates = []\n",
    "        steps = []\n",
    "\n",
    "        for log in logs:\n",
    "            if 'train_loss' in log:\n",
    "                train_losses.append(log['train_loss'])\n",
    "                steps.append(log.get('step', len(train_losses)))\n",
    "            if 'eval_loss' in log:\n",
    "                eval_losses.append(log['eval_loss'])\n",
    "            if 'learning_rate' in log:\n",
    "                learning_rates.append(log['learning_rate'])\n",
    "\n",
    "        plt.subplot(2, 4, 1)\n",
    "        if train_losses:\n",
    "            plt.plot(steps[:len(train_losses)], train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "            plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "        plt.subplot(2, 4, 2)\n",
    "        if eval_losses:\n",
    "            eval_steps = np.linspace(0, len(train_losses), len(eval_losses))\n",
    "            plt.plot(eval_steps, eval_losses, 'r-', linewidth=2, label='Validation Loss')\n",
    "            plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "        plt.subplot(2, 4, 3)\n",
    "        if learning_rates:\n",
    "            plt.plot(learning_rates, 'g-', linewidth=2)\n",
    "            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "    plt.subplot(2, 4, 4)\n",
    "\n",
    "    char_types = {\n",
    "        'Letters': 0,\n",
    "        'Punctuation': 0,\n",
    "        'Special': 0,\n",
    "        'Space': 0\n",
    "    }\n",
    "\n",
    "    for char in vocab_dict.keys():\n",
    "        if char == ' ':\n",
    "            char_types['Space'] += 1\n",
    "        elif char in '[PAD][UNK]':\n",
    "            char_types['Special'] += 1\n",
    "        elif char.isalpha():\n",
    "            char_types['Letters'] += 1\n",
    "        else:\n",
    "            char_types['Punctuation'] += 1\n",
    "\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    plt.pie(char_types.values(), labels=char_types.keys(), autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('Vocabulary Composition', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.subplot(2, 4, 5)\n",
    "\n",
    "    train_size = len(trainer.train_dataset) if hasattr(trainer, 'train_dataset') else 0\n",
    "    eval_size = len(trainer.eval_dataset) if hasattr(trainer, 'eval_dataset') else 0\n",
    "\n",
    "    dataset_info = ['Train Samples', 'Eval Samples', 'Vocab Size']\n",
    "    dataset_values = [train_size, eval_size, len(vocab_dict)]\n",
    "\n",
    "    bars = plt.bar(dataset_info, dataset_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Dataset Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    for bar, value in zip(bars, dataset_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(dataset_values)*0.01,\n",
    "                str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.axis('off')\n",
    "\n",
    "    config_text = f\"\"\"\n",
    "    MODEL CONFIGURATION\n",
    "\n",
    "    Model: {config.MODEL_NAME.split('/')[-1]}\n",
    "    Learning Rate: {config.LEARNING_RATE}\n",
    "    Batch Size: {config.BATCH_SIZE}\n",
    "    Epochs: {config.NUM_EPOCHS}\n",
    "    Max Audio Length: {config.MAX_INPUT_LENGTH // 16000}s\n",
    "\n",
    "    VOCABULARY SAMPLE\n",
    "    {list(vocab_dict.keys())[:15]}...\n",
    "\n",
    "    TRAINING STATUS\n",
    "    Training Completed\n",
    "    Model Saved\n",
    "    Processor Saved\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(0.1, 0.9, config_text, transform=plt.gca().transAxes,\n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.subplot(2, 4, 7)\n",
    "\n",
    "    final_train_loss = train_losses[-1] if train_losses else 0\n",
    "    final_eval_loss = eval_losses[-1] if eval_losses else 0\n",
    "    total_steps = len(train_losses)\n",
    "\n",
    "    performance_data = {\n",
    "        'Metric': ['Final Train Loss', 'Final Eval Loss', 'Total Steps', 'Vocab Coverage'],\n",
    "        'Value': [f'{final_train_loss:.4f}', f'{final_eval_loss:.4f}',\n",
    "                 total_steps, f'{len(vocab_dict)} chars']\n",
    "    }\n",
    "\n",
    "    plt.axis('off')\n",
    "    table_data = []\n",
    "    for metric, value in zip(performance_data['Metric'], performance_data['Value']):\n",
    "        table_data.append([metric, value])\n",
    "\n",
    "    table = plt.table(cellText=table_data,\n",
    "                     colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.6, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "\n",
    "    for i in range(len(table_data) + 1):\n",
    "        for j in range(2):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:  # Header\n",
    "                cell.set_facecolor('#4CAF50')\n",
    "                cell.set_text_props(weight='bold', color='white')\n",
    "            else:  # Data rows\n",
    "                cell.set_facecolor('#E8F5E8' if i % 2 == 0 else 'white')\n",
    "\n",
    "    plt.title('📋 Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    plt.subplot(2, 4, 8)\n",
    "\n",
    "    slovenian_chars = []\n",
    "    regular_chars = []\n",
    "\n",
    "    for char in vocab_dict.keys():\n",
    "        if char in 'čšžČŠŽ':\n",
    "            slovenian_chars.append(char)\n",
    "        elif char.isalpha() and char not in ['[PAD]', '[UNK]']:\n",
    "            regular_chars.append(char)\n",
    "\n",
    "    char_analysis = ['Slovenian Chars', 'Regular Chars', 'Punctuation', 'Special']\n",
    "    char_counts = [len(slovenian_chars), len(regular_chars),\n",
    "                  len([c for c in vocab_dict.keys() if not c.isalnum() and c not in ' [PAD][UNK]']),\n",
    "                  2]\n",
    "\n",
    "    plt.bar(char_analysis, char_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    plt.title('Slovenian Language Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Character Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    for i, count in enumerate(char_counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.suptitle('Wav2Vec2 Slovenian Training Results Dashboard',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "    output_path = \"/kaggle/working/training_results_dashboard.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Dashboard saved as: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def create_simple_summary_table(trainer, vocab_dict, config):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" SLOVENIAN WAV2VEC2 TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    final_train_loss = \"N/A\"\n",
    "    final_eval_loss = \"N/A\"\n",
    "    total_steps = 0\n",
    "\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        logs = trainer.state.log_history\n",
    "        train_losses = [log.get('train_loss') for log in logs if 'train_loss' in log]\n",
    "        eval_losses = [log.get('eval_loss') for log in logs if 'eval_loss' in log]\n",
    "\n",
    "        if train_losses:\n",
    "            final_train_loss = f\"{train_losses[-1]:.4f}\"\n",
    "            total_steps = len(train_losses)\n",
    "        if eval_losses:\n",
    "            final_eval_loss = f\"{eval_losses[-1]:.4f}\"\n",
    "\n",
    "    summary_data = [\n",
    "        (\"Training Date\", datetime.now().strftime(\"%Y-%m-%d %H:%M\")),\n",
    "        (\"Model\", config.MODEL_NAME.split('/')[-1]),\n",
    "        (\"Training Samples\", len(trainer.train_dataset) if hasattr(trainer, 'train_dataset') else 0),\n",
    "        (\"Validation Samples\", len(trainer.eval_dataset) if hasattr(trainer, 'eval_dataset') else 0),\n",
    "        (\"Vocabulary Size\", len(vocab_dict)),\n",
    "        (\"Final Train Loss\", final_train_loss),\n",
    "        (\"Final Eval Loss\", final_eval_loss),\n",
    "        (\"Learning Rate\", config.LEARNING_RATE),\n",
    "        (\"Epochs Completed\", config.NUM_EPOCHS),\n",
    "        (\"Batch Size\", config.BATCH_SIZE),\n",
    "        (\"Total Training Steps\", total_steps),\n",
    "    ]\n",
    "\n",
    "    for label, value in summary_data:\n",
    "        print(f\"{label:<25} : {value}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"VOCABULARY SAMPLE:\")\n",
    "    vocab_sample = list(vocab_dict.keys())[:20]\n",
    "    print(f\"   {vocab_sample}\")\n",
    "    if len(vocab_dict) > 20:\n",
    "        print(f\"   ... and {len(vocab_dict) - 20} more characters\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(f\"Model saved to: {config.OUTPUT_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def show_training_results(trainer, processor, vocab_dict, config):\n",
    "\n",
    "    dashboard_path = create_visual_results_dashboard(trainer, processor, vocab_dict, config)\n",
    "\n",
    "    create_simple_summary_table(trainer, vocab_dict, config)\n",
    "\n",
    "    print(\"\\n SLOVENIAN CHARACTERS FOUND:\")\n",
    "    slovenian_chars = [char for char in vocab_dict.keys() if char in 'čšžćđČŠŽĆĐ']\n",
    "    if slovenian_chars:\n",
    "        print(f\"   {slovenian_chars}\")\n",
    "    else:\n",
    "        print(\"   No Slovenian-specific characters found in this small sample\")\n",
    "\n",
    "    print(f\"\\nVisual dashboard saved as PNG: {dashboard_path}\")\n",
    "    print(\"You can download this image file from Kaggle!\")\n",
    "\n",
    "    return dashboard_path"
   ],
   "id": "372078e387a18fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    Config.NUM_EPOCHS = 2\n",
    "    Config.SAVE_STEPS = 50\n",
    "    Config.EVAL_STEPS = 50\n",
    "    Config.OUTPUT_DIR = \"/working/wav2vec2-test\"\n",
    "    print(small_dataset[0])\n",
    "    return train_the_dataset(small_dataset)"
   ],
   "id": "fbf088ae82b11e36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install hf_xet\n",
    "\n",
    "main_df = pd.concat([dfs['train'], dfs['validated']], ignore_index=True)\n",
    "print(\"After concat:\", main_df.shape)\n",
    "\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(base_clips_dir / \"clips\" / p))\n",
    "\n",
    "main_df = main_df[main_df[\"audio_path\"].apply(lambda x: Path(x).exists())]\n",
    "print(\"After filtering valid paths:\", len(main_df))\n",
    "\n",
    "dataset = prepare_dataset(main_df, sampling_rate=Config.SAMPLING_RATE)\n",
    "\n",
    "trainer, processor = quick_test_training(dataset)\n"
   ],
   "id": "c19955325a42abcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install accelerate>=0.26.0\n",
    "!pip install transformers[torch]\n",
    "\n",
    "def real_quick_test(dataset, max_samples=20):\n",
    "\n",
    "    small_dataset = dataset.select(range(max_samples))\n",
    "\n",
    "    Config.NUM_EPOCHS = 1\n",
    "    Config.BATCH_SIZE = 2\n",
    "    Config.SAVE_STEPS = 10\n",
    "    Config.EVAL_STEPS = 10\n",
    "    Config.OUTPUT_DIR = \"/kaggle/working/wav2vec2-quick\"\n",
    "\n",
    "    trainer, processor = train_the_dataset(small_dataset)\n",
    "    show_training_results(trainer, processor, vocab_dict, Config)\n",
    "\n",
    "\n",
    "    return trainer, processor\n",
    "\n",
    "trainer, processor = real_quick_test(dataset)\n"
   ],
   "id": "4193b0c7db3ff36e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"path\"].head())",
   "id": "6fea33d7e8c61bf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(base_clips_dir)\n",
    "print(list((base_clips_dir / \"clips\").glob(\"*.mp3\"))[:3])"
   ],
   "id": "5dd06fc67844a9e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df[\"audio_path\"] = main_df.apply(get_audio_path, axis=1)",
   "id": "ddb26920675dd6d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "print(main_df[\"audio_path\"].head())\n",
    "print(main_df[\"audio_path\"].apply(lambda x: Path(x).exists()).value_counts())"
   ],
   "id": "df5517744ace4a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(main_df[\"audio_path\"].head())\n",
    "print(main_df[\"audio_path\"].apply(lambda x: Path(x).exists()).value_counts())"
   ],
   "id": "471d866395556f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Available metadata files:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "\n",
    "print(\"Main_df shape after concat:\", main_df.shape)\n"
   ],
   "id": "6048f43ed433dffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[[\"audio_path\", \"sentence\"]].head())",
   "id": "ef44ff2806606da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"audio_path\"].head(10).tolist())",
   "id": "c4a001f6c526c8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for path in main_df[\"audio_path\"].head(10):\n",
    "    print(Path(path).exists())"
   ],
   "id": "7f62c49520b89309",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(base_clips_dir)\n",
    "print(list(Path(base_clips_dir / \"clips\").glob(\"*.mp3\"))[:5])"
   ],
   "id": "121158e413489b17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df = pd.concat([dfs[\"train\"], dfs[\"validated\"]], ignore_index=True)\n",
    "print(main_df.columns)\n",
    "print(main_df[[\"path\", \"sentence\"]].head())\n"
   ],
   "id": "e25ee6eb9b06941",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(base_clips_dir / \"clips\" / p))\n",
    "main_df = main_df[main_df[\"audio_path\"].apply(lambda x: Path(x).exists())]\n",
    "print(f\"✅ Valid audio samples: {len(main_df)}\")"
   ],
   "id": "7d43a6616a5b52ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Pick 5 random entries from main_df\n",
    "sample_paths = main_df[\"path\"].sample(5).tolist()\n",
    "for p in sample_paths:\n",
    "    full_path = base_clips_dir / \"clips\" / p\n",
    "    print(f\"{full_path} → Exists? {Path(full_path).exists()}\")"
   ],
   "id": "a08570affa909393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df = pd.concat([dfs[\"train\"], dfs[\"validated\"]], ignore_index=True)",
   "id": "c9a0090c32c76b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "952aac97d30f64dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"path\"].head())",
   "id": "af7d6496d1d95d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not main_df[\"path\"].iloc[0].endswith(\".mp3\"):\n",
    "    main_df[\"path\"] = main_df[\"path\"].apply(lambda x: x + \".mp3\")"
   ],
   "id": "679108b8b886fc60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(Path(base_clips_dir) / \"clips\" / p))",
   "id": "64915e4f3cbe8e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda x: Path(x).exists())\n",
    "valid_df = main_df[main_df[\"exists\"]]"
   ],
   "id": "ac2b567cbf9ea08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"✅ Valid audio samples: {len(valid_df)}\")\n",
    "print(valid_df[[\"audio_path\", \"sentence\"]].head())"
   ],
   "id": "84e039621fdb289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = prepare_dataset(valid_df, sampling_rate=Config.SAMPLING_RATE)",
   "id": "67f0ad2d69936dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "id": "635a485d629b2914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip uninstall torchcodec -y\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install git+https://github.com/pytorch/torchcodec.git\n"
   ],
   "id": "acadf62b9dd45c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T18:58:57.365976Z",
     "start_time": "2025-07-21T18:58:57.355156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "resampler = T.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "speech_array_16k = resampler(speech_array)\n"
   ],
   "id": "89d1d8e64f4fb173",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T19:03:08.690586Z",
     "start_time": "2025-07-21T19:03:06.314282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "audio_path = 'working\\\\clips\\\\d5effe595a27a3e92d144e1e0a4b9451574082a8dcea3ab59a4270c77b495d0bc87fe388af5e96e709354446aa6d7e7bbbd592a17b042e05ec65a525f031541c.mp3'\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "print(f\"Channels: {audio.channels}, Frame rate: {audio.frame_rate}, Duration: {len(audio)}ms\")"
   ],
   "id": "2898c63c0351f05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels: 1, Frame rate: 48000, Duration: 3408ms\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "944ab1a0b49567fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
