{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "tar_file = r'D:\\PyCharmMiscProject\\slo.zip'\n",
    "\n",
    "extract_dir = 'working'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(tar_file, 'r') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install hf_xet"
   ],
   "id": "df1c6ebc17789a05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:32.248979Z",
     "start_time": "2025-07-24T20:42:29.141375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\"working\")\n",
    "metadata_files = list(base_path.rglob(\"*.tsv\"))\n",
    "\n",
    "train_path = next(p for p in metadata_files if \"train\" in p.name)\n",
    "validated_path = next(p for p in metadata_files if \"validated\" in p.name)\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "validated_df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "main_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "def ensure_mp3_extension(p):\n",
    "    return p if p.endswith(\".mp3\") else p + \".mp3\"\n",
    "\n",
    "main_df[\"path\"] = main_df[\"path\"].apply(ensure_mp3_extension)\n",
    "\n",
    "clips_dir = base_path / \"clips\"\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: (clips_dir / p).as_posix())\n",
    "\n",
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "valid_df = main_df[main_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid audio samples: {len(valid_df)}\")"
   ],
   "id": "1fa438bed83b9215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid audio samples: 1307\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:58.797104Z",
     "start_time": "2025-07-24T20:42:34.323617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    " Wav2Vec2ForCTC,TrainingArguments, Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\n",
    "def prepare_dataset(df, sampling_rate=16000):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df = df.rename(columns={'audio_path': 'audio', 'sentence': 'text'})\n",
    "    ds = Dataset.from_pandas(df[[\"audio\", \"text\"]])\n",
    "    return ds"
   ],
   "id": "5152095a11e16095",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:42:58.841220Z",
     "start_time": "2025-07-24T20:42:58.815633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def load_audio_pydub(path, target_sampling_rate=16000):\n",
    "    audio = AudioSegment.from_file(path)\n",
    "    if audio.frame_rate != target_sampling_rate:\n",
    "        audio = audio.set_frame_rate(target_sampling_rate)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples, target_sampling_rate"
   ],
   "id": "3dd2dc8069360463",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:00.938230Z",
     "start_time": "2025-07-24T20:42:58.860221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n",
    "\n",
    "def preprocess_single_example(example, processor, sampling_rate=16000):\n",
    "        from pydub import AudioSegment\n",
    "        import numpy as np\n",
    "\n",
    "        path = example[\"audio\"]\n",
    "        text = example[\"text\"]\n",
    "\n",
    "        audio = AudioSegment.from_file(path)\n",
    "        if audio.frame_rate != sampling_rate:\n",
    "            audio = audio.set_frame_rate(sampling_rate)\n",
    "        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "        if audio.channels > 1:\n",
    "            samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "\n",
    "        inputs = processor(samples, sampling_rate=sampling_rate, return_attention_mask=True, padding=True)\n",
    "\n",
    "        with processor.as_target_processor():\n",
    "            labels = processor(text).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values[0],\n",
    "            \"attention_mask\": inputs.attention_mask[0],\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "id": "d569a00d1a7acf94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:00.968062Z",
     "start_time": "2025-07-24T20:43:00.961771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "        processor: Wav2Vec2Processor\n",
    "        padding: Union[bool, str] = True\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_features = [{\"input_values\":f[\"input_values\"]} for f in features]\n",
    "            label_features = [{\"input_ids\":f[\"labels\"]} for f in features]\n",
    "\n",
    "            batch = self.processor.pad(\n",
    "                input_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            with self.processor.as_target_processor():\n",
    "                labels_batch = self.processor.pad(\n",
    "                    label_features,\n",
    "                    padding=self.padding,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "            labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = labels\n",
    "\n",
    "            return batch"
   ],
   "id": "ec834c775e79f2df",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "    class Config:\n",
    "        MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "        SAMPLING_RATE = 16000\n",
    "        NUM_EPOCHS = 2\n",
    "        BATCH_SIZE = 4\n",
    "        OUTPUT_DIR = \"./wav2vec2-test\"\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_NAME)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    print(\"Processing dataset...\")\n",
    "    processed_dataset = small_dataset.map(\n",
    "        lambda x: preprocess_single_example(x, processor),\n",
    "        remove_columns=small_dataset.column_names,\n",
    "        desc=\"Processing audio files\"\n",
    "    ).filter(lambda x: x is not None)\n",
    "\n",
    "    print(f\"Processed {len(processed_dataset)} examples\")\n",
    "\n",
    "    split = processed_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_dir=f\"{Config.OUTPUT_DIR}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=5,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=False,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    return trainer, processor"
   ],
   "id": "22ab80cf6717f2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:43:05.632630Z",
     "start_time": "2025-07-24T20:43:00.997679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_NAME, vocab_size=len(processor.tokenizer), ignore_mismatched_sizes=True)\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "26d80235de8b4928",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:46:06.969599Z",
     "start_time": "2025-07-24T20:43:05.649636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = prepare_dataset(valid_df)\n",
    "ds = dataset.map(lambda x: preprocess_single_example(x, processor), remove_columns=dataset.column_names)"
   ],
   "id": "de558bfc2d5d119a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1307/1307 [03:01<00:00,  7.21 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:46:07.174581Z",
     "start_time": "2025-07-24T20:46:07.155399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_ds = ds.train_test_split(test_size=0.1)\n",
    "train_ds = split_ds[\"train\"]\n",
    "eval_ds = split_ds[\"test\"]"
   ],
   "id": "7bbe0fb54aa7b9b9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:46:07.259046Z",
     "start_time": "2025-07-24T20:46:07.235487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-slovene\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    learning_rate=3e-4,\n",
    "    save_total_limit=2,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ],
   "id": "7c8c6e0bd7791e3c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:46:07.306770Z",
     "start_time": "2025-07-24T20:46:07.301187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer(label_str, pred_str),\n",
    "        \"cer\": cer(label_str, pred_str)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n"
   ],
   "id": "28b9591834b86437",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T20:46:07.541282Z",
     "start_time": "2025-07-24T20:46:07.344092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "396f25c197721735",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\AppData\\Local\\Temp\\ipykernel_15112\\448576995.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T21:13:23.933163Z",
     "start_time": "2025-07-24T20:46:07.580128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./wav2vec2-slovene\")\n",
    "processor.save_pretrained(\"./wav2vec2-slovene\")"
   ],
   "id": "d7ed141b1ff39c9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  48/1470 24:33 < 12:39:02, 0.03 it/s, Epoch 0.32/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m trainer.save_model(\u001B[33m\"\u001B[39m\u001B[33m./wav2vec2-slovene\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m processor.save_pretrained(\u001B[33m\"\u001B[39m\u001B[33m./wav2vec2-slovene\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2206\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2204\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2205\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2206\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2207\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2209\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2210\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2211\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2541\u001B[39m context = (\n\u001B[32m   2542\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2543\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2544\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2545\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2546\u001B[39m )\n\u001B[32m   2547\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2548\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2551\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2552\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2553\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2554\u001B[39m ):\n\u001B[32m   2555\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2556\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3797\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   3794\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   3795\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3797\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3799\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2578\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2576\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n\u001B[32m   2577\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2578\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:307\u001B[39m, in \u001B[36mBackwardCFunction.apply\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    301\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    302\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mImplementing both \u001B[39m\u001B[33m'\u001B[39m\u001B[33mbackward\u001B[39m\u001B[33m'\u001B[39m\u001B[33m and \u001B[39m\u001B[33m'\u001B[39m\u001B[33mvjp\u001B[39m\u001B[33m'\u001B[39m\u001B[33m for a custom \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    303\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFunction is not allowed. You should only implement one \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    304\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mof them.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    305\u001B[39m     )\n\u001B[32m    306\u001B[39m user_fn = vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function.vjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43muser_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:303\u001B[39m, in \u001B[36mCheckpointFunction.backward\u001B[39m\u001B[34m(ctx, *args)\u001B[39m\n\u001B[32m    299\u001B[39m     device_autocast_ctx = torch.amp.autocast(\n\u001B[32m    300\u001B[39m         device_type=ctx.device_type, **ctx.device_autocast_kwargs\n\u001B[32m    301\u001B[39m     ) \u001B[38;5;28;01mif\u001B[39;00m torch.amp.is_autocast_available(ctx.device_type) \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext()\n\u001B[32m    302\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.enable_grad(), device_autocast_ctx, torch.amp.autocast(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m, **ctx.cpu_autocast_kwargs):  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m         outputs = \u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdetached_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, torch.Tensor):\n\u001B[32m    306\u001B[39m     outputs = (outputs,)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:703\u001B[39m, in \u001B[36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, output_attentions)\u001B[39m\n\u001B[32m    701\u001B[39m attn_residual = hidden_states\n\u001B[32m    702\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.layer_norm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m hidden_states, attn_weights, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    706\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.dropout(hidden_states)\n\u001B[32m    707\u001B[39m hidden_states = attn_residual + hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:596\u001B[39m, in \u001B[36mWav2Vec2Attention.forward\u001B[39m\u001B[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, **kwargs)\u001B[39m\n\u001B[32m    593\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config._attn_implementation != \u001B[33m\"\u001B[39m\u001B[33meager\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    594\u001B[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m.config._attn_implementation]\n\u001B[32m--> \u001B[39m\u001B[32m596\u001B[39m attn_output, attn_weights = \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    597\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    598\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    599\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    600\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    601\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    602\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    603\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    604\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    609\u001B[39m attn_output = attn_output.reshape(bsz, tgt_len, -\u001B[32m1\u001B[39m).contiguous()\n\u001B[32m    610\u001B[39m attn_output = \u001B[38;5;28mself\u001B[39m.out_proj(attn_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:66\u001B[39m, in \u001B[36msdpa_attention_forward\u001B[39m\u001B[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.jit.is_tracing() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(is_causal, torch.Tensor):\n\u001B[32m     64\u001B[39m     is_causal = is_causal.item()\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m attn_output = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m attn_output = attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m).contiguous()\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m attn_output, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ],
   "id": "8a0cccd3ba67c5fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "86b1f9d6ad147c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T21:13:27.703286Z",
     "start_time": "2025-07-24T21:13:27.699016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Union\n",
    "from pydub import AudioSegment\n",
    "from jiwer import wer\n"
   ],
   "id": "9fc1a50b5ef3f9db",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T21:13:33.707669Z",
     "start_time": "2025-07-24T21:13:28.964144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.freeze_feature_encoder()"
   ],
   "id": "991caae48f1e6dd9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T21:13:33.746697Z",
     "start_time": "2025-07-24T21:13:33.740496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_audio(path):\n",
    "    audio = AudioSegment.from_file(path).set_frame_rate(16000)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples"
   ],
   "id": "54efaca3f0e9933c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T21:13:33.787111Z",
     "start_time": "2025-07-24T21:13:33.781866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(batch):\n",
    "    audio = load_audio(batch[\"audio_path\"])\n",
    "    inputs = processor(audio, sampling_rate=16000, return_attention_mask=True)\n",
    "    with processor.as_target_processor():\n",
    "        labels = processor(batch[\"sentence\"]).input_ids\n",
    "    return {\n",
    "        \"input_values\": inputs[\"input_values\"][0],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "id": "10d8ed7cb56b0ec2",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:35:01.307225Z",
     "start_time": "2025-07-24T22:31:58.486365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = Dataset.from_pandas(valid_df[[\"audio_path\", \"sentence\"]])\n",
    "ds = ds.map(preprocess, remove_columns=ds.column_names)\n",
    "\n",
    "split = ds.train_test_split(test_size=0.1)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]"
   ],
   "id": "cad3b6173227198f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1307 [00:00<?, ? examples/s]D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1307/1307 [03:02<00:00,  7.16 examples/s]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:35:14.549368Z",
     "start_time": "2025-07-24T22:35:14.531700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: Wav2Vec2Processor\n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input = self.processor.pad([{\"input_values\": f[\"input_values\"]} for f in features], return_tensors=\"pt\")\n",
    "        with self.processor.as_target_processor():\n",
    "            labels = self.processor.pad([{\"input_ids\": f[\"labels\"]} for f in features], return_tensors=\"pt\")\n",
    "        input[\"labels\"] = labels[\"input_ids\"].masked_fill(labels[\"attention_mask\"].ne(1), -100)\n",
    "        return input"
   ],
   "id": "11c4364423c3a229",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:35:15.572844Z",
     "start_time": "2025-07-24T22:35:15.563234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    for ref, hyp in zip(label_str[:5], pred_str[:5]):\n",
    "        print(f\"REF : {ref}\")\n",
    "        print(f\"PRED: {hyp}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    error = wer(label_str, pred_str)\n",
    "    return {\"wer\": error}"
   ],
   "id": "149dec59af78e459",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-24T22:35:16.837065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./slovene-model\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        eval_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=1,\n",
    "        report_to=[]\n",
    "    ),\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=DataCollator(processor),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())"
   ],
   "id": "e2a2d18cb9f1fec3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\AppData\\Local\\Temp\\ipykernel_15112\\704549770.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "D:\\PythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:29:39.895152Z",
     "start_time": "2025-07-24T22:29:39.881496Z"
    }
   },
   "cell_type": "code",
   "source": "print(processor.tokenizer.get_vocab().keys())",
   "id": "2f22d6d213bdbc3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['r', 'o', 'x', 'e', 'u', 'š', 'v', 'ž', 'c', 'h', 'i', 'p', 'č', 'w', 'b', 'y', 's', 'd', 'm', 'z', 'l', 'f', 'n', 'j', 'a', 'k', 't', 'g', '|', '[UNK]', '[PAD]', '<s>', '</s>'])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:30:32.910528Z",
     "start_time": "2025-07-24T22:30:32.906268Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset.column_names)",
   "id": "a8a26d13cbfafbd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio', 'text']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7ce786eac4379d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
