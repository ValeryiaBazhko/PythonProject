{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "tar_file = 'sl.tar'\n",
    "\n",
    "\n",
    "extract_dir = 'working'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with tarfile.open(tar_file, 'r') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"Extraction completed.\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install soundfile\n",
    "!pip install hf_xet\n"
   ],
   "id": "3a43386501076567",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:52:17.637155Z",
     "start_time": "2025-07-21T20:52:17.004749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\"working\")\n",
    "metadata_files = list(base_path.rglob(\"*.tsv\"))\n",
    "\n",
    "train_path = next(p for p in metadata_files if \"train\" in p.name)\n",
    "validated_path = next(p for p in metadata_files if \"validated\" in p.name)\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\")\n",
    "validated_df = pd.read_csv(validated_path, sep=\"\\t\")\n",
    "\n",
    "main_df = pd.concat([train_df, validated_df], ignore_index=True)\n",
    "\n",
    "def ensure_mp3_extension(p):\n",
    "    return p if p.endswith(\".mp3\") else p + \".mp3\"\n",
    "\n",
    "main_df[\"path\"] = main_df[\"path\"].apply(ensure_mp3_extension)\n",
    "\n",
    "clips_dir = base_path / \"clips\"\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: (clips_dir / p).as_posix())\n",
    "\n",
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "valid_df = main_df[main_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "print(f\"Valid audio samples: {len(valid_df)}\")\n"
   ],
   "id": "af50c9a68afc198d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid audio samples: 1307\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:52:20.022458Z",
     "start_time": "2025-07-21T20:52:18.952600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(df, sampling_rate=16000):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df = df.rename(columns={'audio_path': 'audio', 'sentence': 'text'})\n",
    "    ds = Dataset.from_pandas(df[[\"audio\", \"text\"]])\n",
    "    return ds\n"
   ],
   "id": "6f5001c5914469c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:52:21.511329Z",
     "start_time": "2025-07-21T20:52:21.493551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def load_audio_pydub(path, target_sampling_rate=16000):\n",
    "    audio = AudioSegment.from_file(path)\n",
    "    if audio.frame_rate != target_sampling_rate:\n",
    "        audio = audio.set_frame_rate(target_sampling_rate)\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels)).mean(axis=1)\n",
    "    return samples, target_sampling_rate"
   ],
   "id": "33417efe7f9c10cf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:52:31.393240Z",
     "start_time": "2025-07-21T20:52:23.271354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n",
    "\n",
    "def preprocess_single_example(example, processor):\n",
    "    try:\n",
    "        audio_path = example[\"audio\"]\n",
    "        text = example[\"text\"]\n",
    "\n",
    "        speech_array, sampling_rate = load_audio_pydub(audio_path)\n",
    "\n",
    "        inputs = processor(speech_array, sampling_rate=sampling_rate, return_attention_mask=True, padding=True)\n",
    "\n",
    "        with processor.as_target_processor():\n",
    "            label_ids = processor(text).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs[\"input_values\"][0],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "            \"labels\": label_ids\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        return None"
   ],
   "id": "b9e30d02a0cda967",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:52:32.403197Z",
     "start_time": "2025-07-21T20:52:32.053771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "    from dataclasses import dataclass\n",
    "    from typing import Dict, List, Union\n",
    "    import torch\n",
    "\n",
    "    class Config:\n",
    "        MODEL_NAME = \"mrshu/wav2vec2-large-xlsr-slovene\"\n",
    "        SAMPLING_RATE = 16000\n",
    "        NUM_EPOCHS = 2\n",
    "        BATCH_SIZE = 4\n",
    "        OUTPUT_DIR = \"./wav2vec2-test\"\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(Config.MODEL_NAME)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        Config.MODEL_NAME,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "    print(\"Processing dataset...\")\n",
    "    processed_dataset = small_dataset.map(\n",
    "        lambda x: preprocess_single_example(x, processor),\n",
    "        remove_columns=small_dataset.column_names,\n",
    "        desc=\"Processing audio files\"\n",
    "    ).filter(lambda x: x is not None)\n",
    "\n",
    "    print(f\"Processed {len(processed_dataset)} examples\")\n",
    "\n",
    "    split = processed_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = split[\"train\"]\n",
    "    eval_dataset = split[\"test\"]\n",
    "\n",
    "    @dataclass\n",
    "    class DataCollatorCTCWithPadding:\n",
    "        processor: Wav2Vec2Processor\n",
    "        padding: Union[bool, str] = True\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_features = []\n",
    "            label_features = []\n",
    "            attention_masks = []\n",
    "\n",
    "            for feature in features:\n",
    "                input_features.append({\"input_values\": feature[\"input_values\"]})\n",
    "                attention_masks.append(feature[\"attention_mask\"])\n",
    "                label_features.append({\"input_ids\": feature[\"labels\"]})\n",
    "\n",
    "            batch = self.processor.pad(\n",
    "                input_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            attention_mask_batch = self.processor.pad(\n",
    "                [{\"input_values\": mask} for mask in attention_masks],\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            batch[\"attention_mask\"] = attention_mask_batch[\"input_values\"]\n",
    "\n",
    "            labels_batch = self.processor.pad(\n",
    "                labels=label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = labels\n",
    "\n",
    "            return batch\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        logging_steps=25,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=False,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    return trainer, processor"
   ],
   "id": "c634cc9654023c78",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 109\u001B[39m\n\u001B[32m    107\u001B[39m     trainer.train()\n\u001B[32m    108\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer, processor\n\u001B[32m--> \u001B[39m\u001B[32m109\u001B[39m trainer, processor = quick_test_training(\u001B[43mdataset\u001B[49m, max_samples=\u001B[32m50\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T21:00:11.781179Z",
     "start_time": "2025-07-21T20:52:59.810547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = prepare_dataset(valid_df, sampling_rate=16000)\n",
    "trainer, processor = quick_test_training(dataset, max_samples=50)"
   ],
   "id": "5681c5826ed14052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick test with 50 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at mrshu/wav2vec2-large-xlsr-slovene and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([31, 1024]) in the checkpoint and torch.Size([33, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([33]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files:   0%|          | 0/50 [00:00<?, ? examples/s]C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Processing audio files: 100%|██████████| 50/50 [00:06<00:00,  7.46 examples/s]\n",
      "Filter: 100%|██████████| 50/50 [00:02<00:00, 19.45 examples/s]\n",
      "C:\\Users\\lerab\\AppData\\Local\\Temp\\ipykernel_32480\\1761338530.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 examples\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\lerab\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 05:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:659] . unexpected pos 1967274880 vs 1967274768",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:965\u001B[39m, in \u001B[36msave\u001B[39m\u001B[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[39m\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[32m--> \u001B[39m\u001B[32m965\u001B[39m     \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m        \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    972\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:1266\u001B[39m, in \u001B[36m_save\u001B[39m\u001B[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[39m\n\u001B[32m   1265\u001B[39m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1266\u001B[39m \u001B[43mzip_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: [enforce fail at inline_container.cc:862] . PytorchStreamWriter failed writing file data/353: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m dataset = prepare_dataset(valid_df, sampling_rate=\u001B[32m16000\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m trainer, processor = \u001B[43mquick_test_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 107\u001B[39m, in \u001B[36mquick_test_training\u001B[39m\u001B[34m(dataset, max_samples)\u001B[39m\n\u001B[32m     97\u001B[39m trainer = Trainer(\n\u001B[32m     98\u001B[39m     model=model,\n\u001B[32m     99\u001B[39m     args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m    103\u001B[39m     data_collator=data_collator,\n\u001B[32m    104\u001B[39m )\n\u001B[32m    106\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m trainer, processor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2206\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2204\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2205\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2206\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2207\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2209\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2210\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2211\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2623\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2621\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.epoch = epoch + (step + \u001B[32m1\u001B[39m + steps_skipped) / steps_in_epoch\n\u001B[32m   2622\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_step_end(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n\u001B[32m-> \u001B[39m\u001B[32m2623\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2624\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2625\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2626\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2627\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2628\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2629\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2630\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstart_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2631\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2632\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2633\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2634\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_substep_end(args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3103\u001B[39m, in \u001B[36mTrainer._maybe_log_save_evaluate\u001B[39m\u001B[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001B[39m\n\u001B[32m   3100\u001B[39m         \u001B[38;5;28mself\u001B[39m.control.should_save = is_new_best_metric\n\u001B[32m   3102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.control.should_save:\n\u001B[32m-> \u001B[39m\u001B[32m3103\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3104\u001B[39m     \u001B[38;5;28mself\u001B[39m.control = \u001B[38;5;28mself\u001B[39m.callback_handler.on_save(\u001B[38;5;28mself\u001B[39m.args, \u001B[38;5;28mself\u001B[39m.state, \u001B[38;5;28mself\u001B[39m.control)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3211\u001B[39m, in \u001B[36mTrainer._save_checkpoint\u001B[39m\u001B[34m(self, model, trial)\u001B[39m\n\u001B[32m   3207\u001B[39m         \u001B[38;5;28mself\u001B[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001B[32m   3209\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.save_only_model:\n\u001B[32m   3210\u001B[39m     \u001B[38;5;66;03m# Save optimizer and scheduler\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3211\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_save_optimizer_and_scheduler\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3212\u001B[39m     \u001B[38;5;28mself\u001B[39m._save_scaler(output_dir)\n\u001B[32m   3213\u001B[39m     \u001B[38;5;66;03m# Save RNG state\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3338\u001B[39m, in \u001B[36mTrainer._save_optimizer_and_scheduler\u001B[39m\u001B[34m(self, output_dir)\u001B[39m\n\u001B[32m   3333\u001B[39m     save_fsdp_optimizer(\n\u001B[32m   3334\u001B[39m         \u001B[38;5;28mself\u001B[39m.accelerator.state.fsdp_plugin, \u001B[38;5;28mself\u001B[39m.accelerator, \u001B[38;5;28mself\u001B[39m.optimizer, \u001B[38;5;28mself\u001B[39m.model, output_dir\n\u001B[32m   3335\u001B[39m     )\n\u001B[32m   3336\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.should_save:\n\u001B[32m   3337\u001B[39m     \u001B[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3338\u001B[39m     \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOPTIMIZER_NAME\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3340\u001B[39m \u001B[38;5;66;03m# Save SCHEDULER & SCALER\u001B[39;00m\n\u001B[32m   3341\u001B[39m is_deepspeed_custom_scheduler = \u001B[38;5;28mself\u001B[39m.is_deepspeed_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m   3342\u001B[39m     \u001B[38;5;28mself\u001B[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001B[32m   3343\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:964\u001B[39m, in \u001B[36msave\u001B[39m\u001B[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[39m\n\u001B[32m    961\u001B[39m     f = os.fspath(f)\n\u001B[32m    963\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[32m--> \u001B[39m\u001B[32m964\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_open_zipfile_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m            \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   (...)\u001B[39m\u001B[32m    970\u001B[39m \u001B[43m            \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mreturn\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\serialization.py:798\u001B[39m, in \u001B[36m_open_zipfile_writer_file.__exit__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    797\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m798\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfile_like\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite_end_of_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    799\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.file_stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    800\u001B[39m         \u001B[38;5;28mself\u001B[39m.file_stream.close()\n",
      "\u001B[31mRuntimeError\u001B[39m: [enforce fail at inline_container.cc:659] . unexpected pos 1967274880 vs 1967274768"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mrshu/wav2vec2-large-xlsr-slovene\")\n"
   ],
   "id": "56292e8000e26c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(output_dir=\"./test\")\n",
    "print(args)"
   ],
   "id": "eb86e8aa03434a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ],
   "id": "28809b43a6762129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Union[int, None] = None\n",
    "    max_length_labels: Union[int, None] = None\n",
    "    pad_to_multiple_of: Union[int, None] = None\n",
    "    pad_to_multiple_of_labels: Union[int, None] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ],
   "id": "1344adf1de2d2be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_noise(audio, noise_factor=0.005):\n",
    "    \"\"\"Add noise to audio for data augmentation\"\"\"\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + noise_factor * noise"
   ],
   "id": "91a3a259e77f5f1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def speed_change(audio, factor=None):\n",
    "    \"\"\"Change speed of audio\"\"\"\n",
    "    if factor is None:\n",
    "        factor = np.random.uniform(0.9, 1.1) # change the values to try\n",
    "    indices = np.round(np.arange(0, len(audio), factor)).astype(int)\n",
    "    indices = indices[indices < len(audio)]\n",
    "    return audio[indices]"
   ],
   "id": "d04da775821e657d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(examples, processor, augment=False):\n",
    "    \"\"\"Preprocess examples\"\"\"\n",
    "\n",
    "    input_values = []\n",
    "    labels = []\n",
    "\n",
    "    for audio_data, text in zip(examples[\"audio\"], examples[\"text\"]):\n",
    "        try:\n",
    "            audio_array = audio_data[\"array\"]\n",
    "\n",
    "            if augment and np.random.random() > 0.5:\n",
    "                aug_type = np.random.choice(['noise', 'speed'])\n",
    "                if aug_type == 'noise':\n",
    "                    audio_array = add_noise(audio_array, noise_factor=0.005)\n",
    "                elif aug_type == 'speed':\n",
    "                    audio_array = speed_change(audio_array)\n",
    "\n",
    "            if len(audio_array) > Config.MAX_INPUT_LENGTH:\n",
    "                audio_array = audio_array[:Config.MAX_INPUT_LENGTH]\n",
    "\n",
    "            input_values.append(audio_array)\n",
    "\n",
    "            normalized_text = normalize_slovenian_text(text)\n",
    "            labels.append(normalized_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(input_values) == 0:\n",
    "        return {\"input_values\": [], \"labels\": []}\n",
    "\n",
    "    inputs = processor(\n",
    "        input_values,\n",
    "        sampling_rate=Config.SAMPLING_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=Config.MAX_INPUT_LENGTH\n",
    "    )\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        label_features = processor(labels).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_values\": inputs.input_values,\n",
    "        \"labels\": label_features\n",
    "    }\n"
   ],
   "id": "684eb105883ae4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_results_to_notebook():\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if os.path.exists(\"/working/wav2vec2-results/training_history.csv\"):\n",
    "        df = pd.read_csv(\"/working/wav2vec2-results/training_history.csv\")\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(df['train_loss'], label='Train Loss')\n",
    "        plt.plot(df['eval_loss'], label='Eval Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(df['learning_rate'])\n",
    "        plt.title('Learning Rate')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Final Train Loss: {df['train_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Final Eval Loss: {df['eval_loss'].iloc[-1]:.4f}\")\n",
    "        print(f\"Total Steps: {len(df)}\")"
   ],
   "id": "d57cd62a75c9e9d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_the_dataset(dataset):\n",
    "\n",
    "    print(f\"Starting training with {len(dataset)} samples\")\n",
    "\n",
    "    train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = train_test[\"train\"]\n",
    "    eval_dataset = train_test[\"test\"]\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "    vocab_dict = create_vocabulary_from_dataset(train_dataset)\n",
    "\n",
    "    import os\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    with open(f\"{Config.OUTPUT_DIR}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_file=f\"{Config.OUTPUT_DIR}/vocab.json\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        word_delimiter_token=\" \"\n",
    "    )\n",
    "\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=Config.SAMPLING_RATE,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    processor = Wav2Vec2Processor(\n",
    "        feature_extractor=feature_extractor,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Preprocessing training data...\")\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        return preprocess_dataset(examples, processor, augment=True)\n",
    "\n",
    "    def preprocess_eval(examples):\n",
    "        return preprocess_dataset(examples, processor, augment=False)\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_train,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_eval,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            Config.MODEL_NAME,\n",
    "            attention_dropout=0.1,\n",
    "            hidden_dropout=0.1,\n",
    "            feat_proj_dropout=0.0,\n",
    "            mask_time_prob=0.05,\n",
    "            layerdrop=0.1,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(processor.tokenizer),\n",
    "            ctc_zero_infinity=True\n",
    "        )\n",
    "\n",
    "    model.freeze_feature_extractor()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=Config.OUTPUT_DIR,\n",
    "            group_by_length=True,\n",
    "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            eval_strategy=\"steps\",\n",
    "            num_train_epochs=Config.NUM_EPOCHS,\n",
    "            fp16=True,\n",
    "            save_steps=Config.SAVE_STEPS,\n",
    "            eval_steps=Config.EVAL_STEPS,\n",
    "            logging_steps=100,\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            weight_decay=0.005,\n",
    "            warmup_steps=Config.WARMUP_STEPS,\n",
    "            save_total_limit=3,\n",
    "            push_to_hub=False,\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "        )\n",
    "\n",
    "    data_collator = DataCollator(processor=processor, padding=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(Config.OUTPUT_DIR)\n",
    "\n",
    "    save_results_to_notebook()\n",
    "\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"Model saved to: {Config.OUTPUT_DIR}\")\n",
    "\n",
    "    return trainer, processor"
   ],
   "id": "147e0df41855e092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(processor_path, model_path, test_audio_path):\n",
    "\n",
    "    processor = Wav2Vec2Processor.from_pretrained(processor_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "\n",
    "    audio, sr = librosa.load(test_audio_path, sr=16000)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription"
   ],
   "id": "da05a6f0538f7386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_visual_results_dashboard(trainer, processor, vocab_dict, config):\n",
    "\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        logs = trainer.state.log_history\n",
    "\n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        learning_rates = []\n",
    "        steps = []\n",
    "\n",
    "        for log in logs:\n",
    "            if 'train_loss' in log:\n",
    "                train_losses.append(log['train_loss'])\n",
    "                steps.append(log.get('step', len(train_losses)))\n",
    "            if 'eval_loss' in log:\n",
    "                eval_losses.append(log['eval_loss'])\n",
    "            if 'learning_rate' in log:\n",
    "                learning_rates.append(log['learning_rate'])\n",
    "\n",
    "        plt.subplot(2, 4, 1)\n",
    "        if train_losses:\n",
    "            plt.plot(steps[:len(train_losses)], train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "            plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "        plt.subplot(2, 4, 2)\n",
    "        if eval_losses:\n",
    "            eval_steps = np.linspace(0, len(train_losses), len(eval_losses))\n",
    "            plt.plot(eval_steps, eval_losses, 'r-', linewidth=2, label='Validation Loss')\n",
    "            plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "        plt.subplot(2, 4, 3)\n",
    "        if learning_rates:\n",
    "            plt.plot(learning_rates, 'g-', linewidth=2)\n",
    "            plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "    plt.subplot(2, 4, 4)\n",
    "\n",
    "    char_types = {\n",
    "        'Letters': 0,\n",
    "        'Punctuation': 0,\n",
    "        'Special': 0,\n",
    "        'Space': 0\n",
    "    }\n",
    "\n",
    "    for char in vocab_dict.keys():\n",
    "        if char == ' ':\n",
    "            char_types['Space'] += 1\n",
    "        elif char in '[PAD][UNK]':\n",
    "            char_types['Special'] += 1\n",
    "        elif char.isalpha():\n",
    "            char_types['Letters'] += 1\n",
    "        else:\n",
    "            char_types['Punctuation'] += 1\n",
    "\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "    plt.pie(char_types.values(), labels=char_types.keys(), autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "    plt.title('Vocabulary Composition', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.subplot(2, 4, 5)\n",
    "\n",
    "    train_size = len(trainer.train_dataset) if hasattr(trainer, 'train_dataset') else 0\n",
    "    eval_size = len(trainer.eval_dataset) if hasattr(trainer, 'eval_dataset') else 0\n",
    "\n",
    "    dataset_info = ['Train Samples', 'Eval Samples', 'Vocab Size']\n",
    "    dataset_values = [train_size, eval_size, len(vocab_dict)]\n",
    "\n",
    "    bars = plt.bar(dataset_info, dataset_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('Dataset Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    for bar, value in zip(bars, dataset_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(dataset_values)*0.01,\n",
    "                str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.axis('off')\n",
    "\n",
    "    config_text = f\"\"\"\n",
    "    MODEL CONFIGURATION\n",
    "\n",
    "    Model: {config.MODEL_NAME.split('/')[-1]}\n",
    "    Learning Rate: {config.LEARNING_RATE}\n",
    "    Batch Size: {config.BATCH_SIZE}\n",
    "    Epochs: {config.NUM_EPOCHS}\n",
    "    Max Audio Length: {config.MAX_INPUT_LENGTH // 16000}s\n",
    "\n",
    "    VOCABULARY SAMPLE\n",
    "    {list(vocab_dict.keys())[:15]}...\n",
    "\n",
    "    TRAINING STATUS\n",
    "    Training Completed\n",
    "    Model Saved\n",
    "    Processor Saved\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(0.1, 0.9, config_text, transform=plt.gca().transAxes,\n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.subplot(2, 4, 7)\n",
    "\n",
    "    final_train_loss = train_losses[-1] if train_losses else 0\n",
    "    final_eval_loss = eval_losses[-1] if eval_losses else 0\n",
    "    total_steps = len(train_losses)\n",
    "\n",
    "    performance_data = {\n",
    "        'Metric': ['Final Train Loss', 'Final Eval Loss', 'Total Steps', 'Vocab Coverage'],\n",
    "        'Value': [f'{final_train_loss:.4f}', f'{final_eval_loss:.4f}',\n",
    "                 total_steps, f'{len(vocab_dict)} chars']\n",
    "    }\n",
    "\n",
    "    plt.axis('off')\n",
    "    table_data = []\n",
    "    for metric, value in zip(performance_data['Metric'], performance_data['Value']):\n",
    "        table_data.append([metric, value])\n",
    "\n",
    "    table = plt.table(cellText=table_data,\n",
    "                     colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.6, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "\n",
    "    for i in range(len(table_data) + 1):\n",
    "        for j in range(2):\n",
    "            cell = table[(i, j)]\n",
    "            if i == 0:  # Header\n",
    "                cell.set_facecolor('#4CAF50')\n",
    "                cell.set_text_props(weight='bold', color='white')\n",
    "            else:  # Data rows\n",
    "                cell.set_facecolor('#E8F5E8' if i % 2 == 0 else 'white')\n",
    "\n",
    "    plt.title('📋 Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    plt.subplot(2, 4, 8)\n",
    "\n",
    "    slovenian_chars = []\n",
    "    regular_chars = []\n",
    "\n",
    "    for char in vocab_dict.keys():\n",
    "        if char in 'čšžČŠŽ':\n",
    "            slovenian_chars.append(char)\n",
    "        elif char.isalpha() and char not in ['[PAD]', '[UNK]']:\n",
    "            regular_chars.append(char)\n",
    "\n",
    "    char_analysis = ['Slovenian Chars', 'Regular Chars', 'Punctuation', 'Special']\n",
    "    char_counts = [len(slovenian_chars), len(regular_chars),\n",
    "                  len([c for c in vocab_dict.keys() if not c.isalnum() and c not in ' [PAD][UNK]']),\n",
    "                  2]\n",
    "\n",
    "    plt.bar(char_analysis, char_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    plt.title('Slovenian Language Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Character Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    for i, count in enumerate(char_counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.suptitle('Wav2Vec2 Slovenian Training Results Dashboard',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "    output_path = \"/kaggle/working/training_results_dashboard.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Dashboard saved as: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def create_simple_summary_table(trainer, vocab_dict, config):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" SLOVENIAN WAV2VEC2 TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    final_train_loss = \"N/A\"\n",
    "    final_eval_loss = \"N/A\"\n",
    "    total_steps = 0\n",
    "\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        logs = trainer.state.log_history\n",
    "        train_losses = [log.get('train_loss') for log in logs if 'train_loss' in log]\n",
    "        eval_losses = [log.get('eval_loss') for log in logs if 'eval_loss' in log]\n",
    "\n",
    "        if train_losses:\n",
    "            final_train_loss = f\"{train_losses[-1]:.4f}\"\n",
    "            total_steps = len(train_losses)\n",
    "        if eval_losses:\n",
    "            final_eval_loss = f\"{eval_losses[-1]:.4f}\"\n",
    "\n",
    "    summary_data = [\n",
    "        (\"Training Date\", datetime.now().strftime(\"%Y-%m-%d %H:%M\")),\n",
    "        (\"Model\", config.MODEL_NAME.split('/')[-1]),\n",
    "        (\"Training Samples\", len(trainer.train_dataset) if hasattr(trainer, 'train_dataset') else 0),\n",
    "        (\"Validation Samples\", len(trainer.eval_dataset) if hasattr(trainer, 'eval_dataset') else 0),\n",
    "        (\"Vocabulary Size\", len(vocab_dict)),\n",
    "        (\"Final Train Loss\", final_train_loss),\n",
    "        (\"Final Eval Loss\", final_eval_loss),\n",
    "        (\"Learning Rate\", config.LEARNING_RATE),\n",
    "        (\"Epochs Completed\", config.NUM_EPOCHS),\n",
    "        (\"Batch Size\", config.BATCH_SIZE),\n",
    "        (\"Total Training Steps\", total_steps),\n",
    "    ]\n",
    "\n",
    "    for label, value in summary_data:\n",
    "        print(f\"{label:<25} : {value}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"VOCABULARY SAMPLE:\")\n",
    "    vocab_sample = list(vocab_dict.keys())[:20]\n",
    "    print(f\"   {vocab_sample}\")\n",
    "    if len(vocab_dict) > 20:\n",
    "        print(f\"   ... and {len(vocab_dict) - 20} more characters\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(f\"Model saved to: {config.OUTPUT_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def show_training_results(trainer, processor, vocab_dict, config):\n",
    "\n",
    "    dashboard_path = create_visual_results_dashboard(trainer, processor, vocab_dict, config)\n",
    "\n",
    "    create_simple_summary_table(trainer, vocab_dict, config)\n",
    "\n",
    "    print(\"\\n SLOVENIAN CHARACTERS FOUND:\")\n",
    "    slovenian_chars = [char for char in vocab_dict.keys() if char in 'čšžćđČŠŽĆĐ']\n",
    "    if slovenian_chars:\n",
    "        print(f\"   {slovenian_chars}\")\n",
    "    else:\n",
    "        print(\"   No Slovenian-specific characters found in this small sample\")\n",
    "\n",
    "    print(f\"\\nVisual dashboard saved as PNG: {dashboard_path}\")\n",
    "    print(\"You can download this image file from Kaggle!\")\n",
    "\n",
    "    return dashboard_path"
   ],
   "id": "372078e387a18fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def quick_test_training(dataset, max_samples=100):\n",
    "\n",
    "    print(f\"Running quick test with {max_samples} samples...\")\n",
    "\n",
    "    small_dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    Config.NUM_EPOCHS = 2\n",
    "    Config.SAVE_STEPS = 50\n",
    "    Config.EVAL_STEPS = 50\n",
    "    Config.OUTPUT_DIR = \"/working/wav2vec2-test\"\n",
    "    print(small_dataset[0])\n",
    "    return train_the_dataset(small_dataset)"
   ],
   "id": "fbf088ae82b11e36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install hf_xet\n",
    "\n",
    "main_df = pd.concat([dfs['train'], dfs['validated']], ignore_index=True)\n",
    "print(\"After concat:\", main_df.shape)\n",
    "\n",
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(base_clips_dir / \"clips\" / p))\n",
    "\n",
    "main_df = main_df[main_df[\"audio_path\"].apply(lambda x: Path(x).exists())]\n",
    "print(\"After filtering valid paths:\", len(main_df))\n",
    "\n",
    "dataset = prepare_dataset(main_df, sampling_rate=Config.SAMPLING_RATE)\n",
    "\n",
    "trainer, processor = quick_test_training(dataset)\n"
   ],
   "id": "c19955325a42abcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install accelerate>=0.26.0\n",
    "!pip install transformers[torch]\n",
    "\n",
    "def real_quick_test(dataset, max_samples=20):\n",
    "\n",
    "    small_dataset = dataset.select(range(max_samples))\n",
    "\n",
    "    Config.NUM_EPOCHS = 1\n",
    "    Config.BATCH_SIZE = 2\n",
    "    Config.SAVE_STEPS = 10\n",
    "    Config.EVAL_STEPS = 10\n",
    "    Config.OUTPUT_DIR = \"/kaggle/working/wav2vec2-quick\"\n",
    "\n",
    "    trainer, processor = train_the_dataset(small_dataset)\n",
    "    show_training_results(trainer, processor, vocab_dict, Config)\n",
    "\n",
    "\n",
    "    return trainer, processor\n",
    "\n",
    "trainer, processor = real_quick_test(dataset)\n"
   ],
   "id": "4193b0c7db3ff36e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"path\"].head())",
   "id": "6fea33d7e8c61bf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(base_clips_dir)\n",
    "print(list((base_clips_dir / \"clips\").glob(\"*.mp3\"))[:3])"
   ],
   "id": "5dd06fc67844a9e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df[\"audio_path\"] = main_df.apply(get_audio_path, axis=1)",
   "id": "ddb26920675dd6d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "print(main_df[\"audio_path\"].head())\n",
    "print(main_df[\"audio_path\"].apply(lambda x: Path(x).exists()).value_counts())"
   ],
   "id": "df5517744ace4a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(main_df[\"audio_path\"].head())\n",
    "print(main_df[\"audio_path\"].apply(lambda x: Path(x).exists()).value_counts())"
   ],
   "id": "471d866395556f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Available metadata files:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "\n",
    "print(\"Main_df shape after concat:\", main_df.shape)\n"
   ],
   "id": "6048f43ed433dffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[[\"audio_path\", \"sentence\"]].head())",
   "id": "ef44ff2806606da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"audio_path\"].head(10).tolist())",
   "id": "c4a001f6c526c8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for path in main_df[\"audio_path\"].head(10):\n",
    "    print(Path(path).exists())"
   ],
   "id": "7f62c49520b89309",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(base_clips_dir)\n",
    "print(list(Path(base_clips_dir / \"clips\").glob(\"*.mp3\"))[:5])"
   ],
   "id": "121158e413489b17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df = pd.concat([dfs[\"train\"], dfs[\"validated\"]], ignore_index=True)\n",
    "print(main_df.columns)\n",
    "print(main_df[[\"path\", \"sentence\"]].head())\n"
   ],
   "id": "e25ee6eb9b06941",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(base_clips_dir / \"clips\" / p))\n",
    "main_df = main_df[main_df[\"audio_path\"].apply(lambda x: Path(x).exists())]\n",
    "print(f\"✅ Valid audio samples: {len(main_df)}\")"
   ],
   "id": "7d43a6616a5b52ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Pick 5 random entries from main_df\n",
    "sample_paths = main_df[\"path\"].sample(5).tolist()\n",
    "for p in sample_paths:\n",
    "    full_path = base_clips_dir / \"clips\" / p\n",
    "    print(f\"{full_path} → Exists? {Path(full_path).exists()}\")"
   ],
   "id": "a08570affa909393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df = pd.concat([dfs[\"train\"], dfs[\"validated\"]], ignore_index=True)",
   "id": "c9a0090c32c76b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "952aac97d30f64dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(main_df[\"path\"].head())",
   "id": "af7d6496d1d95d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not main_df[\"path\"].iloc[0].endswith(\".mp3\"):\n",
    "    main_df[\"path\"] = main_df[\"path\"].apply(lambda x: x + \".mp3\")"
   ],
   "id": "679108b8b886fc60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df[\"audio_path\"] = main_df[\"path\"].apply(lambda p: str(Path(base_clips_dir) / \"clips\" / p))",
   "id": "64915e4f3cbe8e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_df[\"exists\"] = main_df[\"audio_path\"].apply(lambda x: Path(x).exists())\n",
    "valid_df = main_df[main_df[\"exists\"]]"
   ],
   "id": "ac2b567cbf9ea08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\" Valid audio samples: {len(valid_df)}\")\n",
    "print(valid_df[[\"audio_path\", \"sentence\"]].head())"
   ],
   "id": "84e039621fdb289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = prepare_dataset(valid_df, sampling_rate=Config.SAMPLING_RATE)",
   "id": "67f0ad2d69936dfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "id": "635a485d629b2914",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip uninstall torchcodec -y\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install git+https://github.com/pytorch/torchcodec.git\n"
   ],
   "id": "acadf62b9dd45c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "resampler = T.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "speech_array_16k = resampler(speech_array)\n"
   ],
   "id": "89d1d8e64f4fb173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "audio_path = 'working\\\\clips\\\\d5effe595a27a3e92d144e1e0a4b9451574082a8dcea3ab59a4270c77b495d0bc87fe388af5e96e709354446aa6d7e7bbbd592a17b042e05ec65a525f031541c.mp3'\n",
    "audio = AudioSegment.from_file(audio_path)\n",
    "print(f\"Channels: {audio.channels}, Frame rate: {audio.frame_rate}, Duration: {len(audio)}ms\")"
   ],
   "id": "2898c63c0351f05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, let's debug what's in your dataset\n",
    "print(\"Dataset columns:\", dataset.column_names)\n",
    "print(\"First few examples:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    example = dataset[i]\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  Audio path: {example['audio']}\")\n",
    "    print(f\"  Text: {example['text'][:50]}...\")  # First 50 chars of text\n",
    "    print(f\"  Path exists: {Path(example['audio']).exists()}\")\n",
    "    print()"
   ],
   "id": "944ab1a0b49567fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "37fadef79618d98f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
